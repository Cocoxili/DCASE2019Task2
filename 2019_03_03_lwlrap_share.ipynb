{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Aq8VVHsTohAy"
   },
   "source": [
    "# 2019-03-03_lwlrap-share\n",
    "\n",
    "Reference implementation of l$\\omega$lrap both natively and using sklearn.metrics.\n",
    "\n",
    "Dan Ellis dpwe@google.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IHIGP6A9ogyb"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xffu7w5t0YFa"
   },
   "outputs": [],
   "source": [
    "# Core calculation of label precisions for one test sample.\n",
    "\n",
    "def _one_sample_positive_class_precisions(scores, truth):\n",
    "  \"\"\"Calculate precisions for each true class for a single sample.\n",
    "  \n",
    "  Args:\n",
    "    scores: np.array of (num_classes,) giving the individual classifier scores.\n",
    "    truth: np.array of (num_classes,) bools indicating which classes are true.\n",
    "\n",
    "  Returns:\n",
    "    pos_class_indices: np.array of indices of the true classes for this sample.\n",
    "    pos_class_precisions: np.array of precisions corresponding to each of those\n",
    "      classes.\n",
    "  \"\"\"\n",
    "  num_classes = scores.shape[0]\n",
    "  pos_class_indices = np.flatnonzero(truth > 0)\n",
    "  # Only calculate precisions if there are some true classes.\n",
    "  if not len(pos_class_indices):\n",
    "    return pos_class_indices, np.zeros(0)\n",
    "  # Retrieval list of classes for this sample. \n",
    "  retrieved_classes = np.argsort(scores)[::-1]\n",
    "  # class_rankings[top_scoring_class_index] == 0 etc.\n",
    "  class_rankings = np.zeros(num_classes, dtype=np.int)\n",
    "  class_rankings[retrieved_classes] = range(num_classes)\n",
    "  # Which of these is a true label?\n",
    "  retrieved_class_true = np.zeros(num_classes, dtype=np.bool)\n",
    "  retrieved_class_true[class_rankings[pos_class_indices]] = True\n",
    "  # Num hits for every truncated retrieval list.\n",
    "  retrieved_cumulative_hits = np.cumsum(retrieved_class_true)\n",
    "  # Precision of retrieval list truncated at each hit, in order of pos_labels.\n",
    "  precision_at_hits = (\n",
    "      retrieved_cumulative_hits[class_rankings[pos_class_indices]] / \n",
    "      (1 + class_rankings[pos_class_indices].astype(np.float)))\n",
    "  return pos_class_indices, precision_at_hits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5HfziEYbodWk"
   },
   "outputs": [],
   "source": [
    "# All-in-one calculation of per-class lwlrap.\n",
    "\n",
    "def calculate_per_class_lwlrap(truth, scores):\n",
    "  \"\"\"Calculate label-weighted label-ranking average precision.\n",
    "  \n",
    "  Arguments:\n",
    "    truth: np.array of (num_samples, num_classes) giving boolean ground-truth\n",
    "      of presence of that class in that sample.\n",
    "    scores: np.array of (num_samples, num_classes) giving the classifier-under-\n",
    "      test's real-valued score for each class for each sample.\n",
    "  \n",
    "  Returns:\n",
    "    per_class_lwlrap: np.array of (num_classes,) giving the lwlrap for each \n",
    "      class.\n",
    "    weight_per_class: np.array of (num_classes,) giving the prior of each \n",
    "      class within the truth labels.  Then the overall unbalanced lwlrap is \n",
    "      simply np.sum(per_class_lwlrap * weight_per_class)\n",
    "  \"\"\"\n",
    "  assert truth.shape == scores.shape\n",
    "  num_samples, num_classes = scores.shape\n",
    "  # Space to store a distinct precision value for each class on each sample.\n",
    "  # Only the classes that are true for each sample will be filled in.\n",
    "  precisions_for_samples_by_classes = np.zeros((num_samples, num_classes))\n",
    "  for sample_num in range(num_samples):\n",
    "    pos_class_indices, precision_at_hits = (\n",
    "      _one_sample_positive_class_precisions(scores[sample_num, :], \n",
    "                                            truth[sample_num, :]))\n",
    "    precisions_for_samples_by_classes[sample_num, pos_class_indices] = (\n",
    "        precision_at_hits)\n",
    "  labels_per_class = np.sum(truth > 0, axis=0)\n",
    "  weight_per_class = labels_per_class / float(np.sum(labels_per_class))\n",
    "  # Form average of each column, i.e. all the precisions assigned to labels in\n",
    "  # a particular class.\n",
    "  per_class_lwlrap = (np.sum(precisions_for_samples_by_classes, axis=0) / \n",
    "                      np.maximum(1, labels_per_class))\n",
    "  # overall_lwlrap = simple average of all the actual per-class, per-sample precisions\n",
    "  #                = np.sum(precisions_for_samples_by_classes) / np.sum(precisions_for_samples_by_classes > 0)\n",
    "  #           also = weighted mean of per-class lwlraps, weighted by class label prior across samples\n",
    "  #                = np.sum(per_class_lwlrap * weight_per_class)\n",
    "  return per_class_lwlrap, weight_per_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "52LPXQNPppex"
   },
   "outputs": [],
   "source": [
    "# Calculate the overall lwlrap using sklearn.metrics function.\n",
    "\n",
    "def calculate_overall_lwlrap_sklearn(truth, scores):\n",
    "  \"\"\"Calculate the overall lwlrap using sklearn.metrics.lrap.\"\"\"\n",
    "  # sklearn doesn't correctly apply weighting to samples with no labels, so just skip them.\n",
    "  sample_weight = np.sum(truth > 0, axis=1)\n",
    "  print(sample_weight)\n",
    "  nonzero_weight_sample_indices = np.flatnonzero(sample_weight > 0)\n",
    "  print(sample_weight[nonzero_weight_sample_indices])\n",
    "  overall_lwlrap = sklearn.metrics.label_ranking_average_precision_score(\n",
    "      truth[nonzero_weight_sample_indices, :] > 0, \n",
    "      scores[nonzero_weight_sample_indices, :], \n",
    "      sample_weight=sample_weight[nonzero_weight_sample_indices])\n",
    "  return overall_lwlrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FJv0Rtqfsu3X"
   },
   "outputs": [],
   "source": [
    "# Accumulator object version.\n",
    "\n",
    "class lwlrap_accumulator(object):\n",
    "  \"\"\"Accumulate batches of test samples into per-class and overall lwlrap.\"\"\"  \n",
    "\n",
    "  def __init__(self):\n",
    "    self.num_classes = 0\n",
    "    self.total_num_samples = 0\n",
    "  \n",
    "  def accumulate_samples(self, batch_truth, batch_scores):\n",
    "    \"\"\"Cumulate a new batch of samples into the metric.\n",
    "    \n",
    "    Args:\n",
    "      truth: np.array of (num_samples, num_classes) giving boolean\n",
    "        ground-truth of presence of that class in that sample for this batch.\n",
    "      scores: np.array of (num_samples, num_classes) giving the \n",
    "        classifier-under-test's real-valued score for each class for each\n",
    "        sample.\n",
    "    \"\"\"\n",
    "    assert batch_scores.shape == batch_truth.shape\n",
    "    num_samples, num_classes = batch_truth.shape\n",
    "    if not self.num_classes:\n",
    "      self.num_classes = num_classes\n",
    "      self._per_class_cumulative_precision = np.zeros(self.num_classes)\n",
    "      self._per_class_cumulative_count = np.zeros(self.num_classes, \n",
    "                                                  dtype=np.int)\n",
    "    assert num_classes == self.num_classes\n",
    "    for truth, scores in zip(batch_truth, batch_scores):\n",
    "      pos_class_indices, precision_at_hits = (\n",
    "        _one_sample_positive_class_precisions(scores, truth))\n",
    "      self._per_class_cumulative_precision[pos_class_indices] += (\n",
    "        precision_at_hits)\n",
    "      self._per_class_cumulative_count[pos_class_indices] += 1\n",
    "    self.total_num_samples += num_samples\n",
    "\n",
    "  def per_class_lwlrap(self):\n",
    "    \"\"\"Return a vector of the per-class lwlraps for the accumulated samples.\"\"\"\n",
    "    return (self._per_class_cumulative_precision / \n",
    "            np.maximum(1, self._per_class_cumulative_count))\n",
    "\n",
    "  def per_class_weight(self):\n",
    "    \"\"\"Return a normalized weight vector for the contributions of each class.\"\"\"\n",
    "    return (self._per_class_cumulative_count / \n",
    "            float(np.sum(self._per_class_cumulative_count)))\n",
    "\n",
    "  def overall_lwlrap(self):\n",
    "    \"\"\"Return the scalar overall lwlrap for cumulated samples.\"\"\"\n",
    "    return np.sum(self.per_class_lwlrap() * self.per_class_weight())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cRCaCIb9oguU"
   },
   "outputs": [],
   "source": [
    "# Random test data.\n",
    "num_samples = 100\n",
    "num_labels = 20\n",
    "\n",
    "truth = np.random.rand(num_samples, num_labels) > 0.5\n",
    "# Ensure at least some samples with no truth labels.\n",
    "truth[0:1, :] = False\n",
    "\n",
    "scores = np.random.rand(num_samples, num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "FkXJOnMiqQVa",
    "outputId": "6cc526f9-37cd-4cc2-ce9c-613375de37a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lwlrap from per-class values= 0.5809144975639644\n",
      "lwlrap from sklearn.metrics = 0.5809144975639644\n"
     ]
    }
   ],
   "source": [
    "per_class_lwlrap, weight_per_class = calculate_per_class_lwlrap(truth, scores)\n",
    "print(\"lwlrap from per-class values=\", np.sum(per_class_lwlrap * weight_per_class))\n",
    "print(\"lwlrap from sklearn.metrics =\", calculate_overall_lwlrap_sklearn(truth, scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "TVtzP2u-0FvO",
    "outputId": "bd7a2515-484e-40e2-a0b6-a7e88f5c3c9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cumulative_lwlrap= 0.5809144975639644\n",
      "total_num_samples= 100\n"
     ]
    }
   ],
   "source": [
    "# Test of accumulator version.\n",
    "accumulator = lwlrap_accumulator()\n",
    "batch_size = 12\n",
    "for base_sample in range(0, scores.shape[0], batch_size):\n",
    "  accumulator.accumulate_samples(\n",
    "      truth[base_sample : base_sample + batch_size, :], \n",
    "      scores[base_sample : base_sample + batch_size, :])\n",
    "print(\"cumulative_lwlrap=\", accumulator.overall_lwlrap())\n",
    "print(\"total_num_samples=\", accumulator.total_num_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gJhxzy7_0PRE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0 11 12 10  7  8 10 11 11 12 10  9 12  7 13 14 10 12 11  9 10 10 10 13\n",
      " 10 13  8  6 10 13 11 10 13  9  9 10 11  7  9  8 11 11 11 12 12 11  9 12\n",
      " 12 11 10  6 12 12 13 13 12  8  6  9  8 11 12 10  9 10 16 12  6  6 12  5\n",
      " 11 10 10 10 12  7  9  8 10 10 12 12 10  9 12 11  8  8  9 11 12  9 10  8\n",
      "  6  9  7 10]\n",
      "[11 12 10  7  8 10 11 11 12 10  9 12  7 13 14 10 12 11  9 10 10 10 13 10\n",
      " 13  8  6 10 13 11 10 13  9  9 10 11  7  9  8 11 11 11 12 12 11  9 12 12\n",
      " 11 10  6 12 12 13 13 12  8  6  9  8 11 12 10  9 10 16 12  6  6 12  5 11\n",
      " 10 10 10 12  7  9  8 10 10 12 12 10  9 12 11  8  8  9 11 12  9 10  8  6\n",
      "  9  7 10]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5809144975639644"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# truth\n",
    "# scores\n",
    "calculate_overall_lwlrap_sklearn(truth, scores)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.zeros(4,4)\n",
    "b = torch.ones(2, 4)\n",
    "a[0:2,:] = b\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(a.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fname</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>x</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>z</th>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       b  c\n",
       "fname      \n",
       "x      1  2\n",
       "y      4  5\n",
       "z      6  7"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "h = ['b','c']\n",
    "a = pd.DataFrame(index=['x','y','z'], data=[[1,2], [4,5], [6,7]])\n",
    "# h1= ['a'] + h\n",
    "a.columns = h\n",
    "a.index.name = 'fname'\n",
    "a\n",
    "# df = pd.DataFrame(index=files_name, data=prediction)\n",
    "# path = os.path.join(config.prediction_dir, file)\n",
    "# df.to_csv(path, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "2019-03-03_lwlrap-share.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
